# Project Implementation Description
1. The *Confluent Datagen - Clickstreaming* is a live data source that continuously populates the web-application-clickstream Kafka Topic with web application traffic data.
2. The Confluent HTTP Sink (*clickhouse-webclicks*) writes the topic data to the Clickhouse *default.webclicks* table.
3. Clickhouse Views are generated by dbt automation to tables in a new Clickhouse database called *webclicks_dbt*. The dbt automation includes eight tests for expected values (ex: non-null values, percentages between 0 and 100 inclusive, etc.)
4. A Preset dataset is created for each of the Clickhouse Views.
5. A Preset Dashboard is created from each of the datasets.
6. All of these components are hosted in cloud environments maintained by the respective vendors.
## Confluent Datagen Source Data Schema
```json
{
  "$id": "http://example.com/myURI.schema.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "additionalProperties": false,
  "description": "Describe the fields in our web click traffic records.",
  "properties": {
    "_time": {
      "description": "The timestamp in UNIX epoch format when the request was made, as an integer.",
      "type": "integer"
    },
    "agent": {
      "description": "Detailed information about the client application (ex: web browser) accessing the site.",
      "type": "string"
    },
    "bytes": {
      "description": "Amount of data transferred during this requestin bytes.",
      "type": "string"
    },
    "ip": {
      "description": "The IP address making the web application request.",
      "type": "string"
    },
    "referrer": {
      "description": "The site that referred the client application (ex: web browser) to our site.",
      "type": "string"
    },
    "remote_user": {
      "description": "The string type is used for strings of text.",
      "type": "string"
    },
    "request": {
      "description": "The resource requested by the client application (ex: web browser).",
      "type": "string"
    },
    "status": {
      "description": "The HTTP status code in the response.",
      "type": "string"
    },
    "time": {
      "description": "The timestamp in UNIX epoch format when the request was made, as a string.",
      "type": "string"
    },
    "userid": {
      "description": "The unique identifier of associated with the request.",
      "type": "integer"
    }
  },
  "title": "WebClickTraffic",
  "type": "object"
}
```
## dbt Tests Implemented
### assert_users_models_acceptable_values.yml
1. The IP address field in the top 10 users by clicks table is not null.
    - This would indicate a corrupted records in the source data and/or problems the data transformation if this field is null.
2. The IP address field in the top 10 users by clicks table is unique.
    - This would indicate potential duplicate data in the source and/or a problem with the data transformation if there are duplicate IP addresses in the top 10 list.
### assert_weberrors_models_acceptable_values.yml
3. The hourly 404 errors table is not null
4. The hourly non-404 errors table is not null
5. The percentage of 404 errors is greater than or equal to 0
6. The percentage of 404 errors is less than or equal to 100
7. The percentage of non-404 errors is greater than or equal to 0
8. The percentage of non-404 errors is less than or equal to 100
# Screenshots
## Confluent Cloud
### Environment View
![../img/confluent-environment.png](../img/confluent-environment.png)
### Kafka Topic
![../img/confluent-Kafka-topic.png](../img/confluent-Kafka-topic.png)
## Clickhouse
### Service View
![../img/clickhouse-00-service.png](../img/clickhouse-00-service.png)
### Data Sink Results View
![../img/clickhouse-01-webclicks-kafka-import.png](../img/clickhouse-01-webclicks-kafka-import.png)
### dbt-Generated Model Views
![../img/clickhouse-02-views.png](../img/clickhouse-02-views.png)
## Preset
### Webclicks Analytics Dashboard
![../img/preset-dashboard.png](../img/preset-dashboard.png)